---
title: "Partition MCMC applied to a test data set"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What is this for?

This shows an example using dagmc on several bnlearn test data sets.

## Setup

```{r setup, message = FALSE}
library(dagmc)
library(bnlearn)
library(Rgraphviz)
library(dplyr)
library(ggplot2)
```

Get test problem from the bnlearn library.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
# Select a problem to test.

# data <- bnlearn::alarm[1:100, ]
# type <- 'bde'
# modelstring <- paste0("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
#   "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
#   "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
#   "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
#   "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG]",
#   "[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]")
# true_graph <- bnlearn::model2network(modelstring)

# # Asia
# data <- bnlearn::asia[1:1000, ]
# type <- 'bde'
# arc_set <- matrix(c('A', 'T',
#                     'T', 'E',
#                     'S', 'B',
#                     'B', 'D',
#                     'L', 'E',
#                     'E', 'D',
#                     'E', 'X'
#                     ),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))

# # Cornary
# data <- bnlearn::coronary
# type <- 'bde'
# arc_set <- matrix(c('Smoking', 'M. Work',
#                     'Smoking', 'Pressure',
#                     'Smoking', 'P. Work',
#                     'M. Work', 'Family',
#                     'M. Work', 'Proteins',
#                     'Pressure', 'Proteins'
#                     ),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))


# # BNlearn learning.test. A discretised problem.
# data <- bnlearn::learning.test[1:1000, ]
# type <- 'bde'
# arc_set <- matrix(c('F', 'E',
#                     'A', 'D',
#                     'B', 'A',
#                     'B', 'E',
#                     'C', 'D'),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))

# BNlearn Gaussian test. A gaussian problem.
data <- bnlearn::gaussian.test[1:500, ]
type <- 'bge'
true_graph <- bnlearn::empty.graph(names(data))
arc_set <- matrix(c('A', 'F',
                    'A', 'C',
                    'B', 'C',
                    'B', 'D',
                    'G', 'F',
                    'D', 'F',
                    'E', 'F'),
                  ncol = 2, byrow = TRUE,
                  dimnames = list(NULL, c('from', 'to')))
```

Plot true graph.

```{r, error = TRUE}
true_graph <- bnlearn::empty.graph(names(data))
bnlearn::arcs(true_graph) <- arc_set
true_adj <- true_graph |>
   bnlearn::as.igraph() |>
   igraph::as_adjacency_matrix() |>
   as.matrix()
```

Show the true DAG from which the data were sampled.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
plot(bnlearn::cpdag(true_graph))
```

## Posterior sampling.

The aim of dagmc is to sample from the posterior distribution of DAGs. This is
what is done here.

First, a function that calculates the score of a given (node, parents) is
setup.

```{r, error = TRUE}
max_parents <- Inf
scorer <- CreateScorer(scorer = BNLearnScorer,
                       data = data,
                       type = type,
                       max_parents = max_parents,
                       blacklist = NULL,
                       whitelist = NULL,
                       cache = FALSE)
blacklist <- GetLowestPairwiseScoringEdges(scorer, n_retain = 20)
system.time(
  scorer <- CreateScorer(scorer = BNLearnScorer,
                         data = data, type = type,
                         max_parents = max_parents,
                         blacklist = blacklist,
                         whitelist = NULL,
                         cache = TRUE)
)
```

I then sample a random DAG from the prior space as an initial state for the MCMC
and get 'n_results' from the sampler. This procedure is done several times
to check that similar results are being reached on each run.

```{r, error = TRUE}
set.seed(1)
n_results <- 10000
n_runs <- 4

init_partitions <- list()
for (i in 1:n_runs) {
  init_dag <- UniformlySampleDAG(names(data))
  
  # Remove edges that disobey white/black listing. I think this breaks the 
  # uniformity from the prior but this will do for now.
  if (!is.null(scorer$blacklist))
    init_dag[which(scorer$blacklist)] <- 0
  
  if (!is.null(scorer$whitelist))
    init_dag[which(scorer$whitelist)] <- 1
  
  init_partitions[[i]] <- GetPartitionedNodesFromAdjacencyMatrix(init_dag)
}

system.time(
  chains <- SampleChains(n_results, init_partitions,
                         transition = PartitionMCMC(),
                         scorer = scorer,
                         n_parallel_chains = n_runs)
)
```

## Check the chains for convergence

Plot trace of the labelled partition log scores. The aim is to check that the 
MCMC is not getting stuck in local maxima for a large number of saves and that 
multiple chains reach the same maxima.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
PlotScoreTrace(chains, ylab = 'log(partition score)', type = 'l')
```

Plot the score trace after removing a burn-in.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
n_burnin <- 500
eq_chains <- chains[(1 + n_burnin):n_results]
PlotScoreTrace(eq_chains, ylab = 'log(partition score)', type = 'l')
```

Plot DAG scores. Involes sampling a DAG per iteration and plotting the corresponding
score.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
eq_dag_chains <- PartitiontoDAG(eq_chains, scorer)
PlotScoreTrace(eq_dag_chains, attribute = 'log_score', ylab = 'log(DAG score)', type = 'l')
```

## Analysing acceptance rates

Acceptance per proposal. This is often incredibly low, so I do wonder if we've
explored enough of the posterior space. Increasing the temperature of the score
is likely to help.

```{r, error = TRUE}
prop_accept <- CalculateAcceptanceRates(eq_chains, group_by = 'proposal_used')
knitr::kable(prop_accept)
```

## Collect most probable CPDAGs and perform inference for features of interest

It's often more robust to look at the probabilities of CPDAGs, where edges can
go both ways.

```{r, error = TRUE}
eq_cpdag_chains <- DAGtoCPDAG(eq_dag_chains)
flat_eq_cpdag_chains <- FlattenChains(eq_cpdag_chains)
cpdag_collection <- CollectUniqueObjects(flat_eq_cpdag_chains)
```

Get most probable DAGs. I'm just plotting the CPDAG of the first map DAG found here.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
map_dags <- GetMAP(cpdag_collection)
bn_map_dag <- toBNLearn(map_dags$state[[1]])
plot(bnlearn::cpdag(bn_map_dag))
```



Calculate marginalised pairwise edge probabilities using a CPDAG collection. 

```{r, error = TRUE}
p_edge <- CalculateEdgeProbabilities(cpdag_collection)
knitr::kable(round(p_edge, 3))
```
