---
title: "Three Node Example"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{three_node_example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What is this for?

This shows inference on a data set drawn from a fully connected three node DAG.
As the number of possible three node DAGs is 25, this allows for a direct 
comparison between the expected and actual frequency of samples for each DAG.

```{r setup}
library(dagmc)
library(magrittr)
```

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
nodes <- LETTERS[1:3]
set.seed(2)
true_dag <- UniformlySampleDAG(nodes)

true_graph <- bnlearn::empty.graph(nodes)
bnlearn::amat(true_graph) <- true_dag
plot(true_graph)
```

Create conditional probability tables.

```{r, error = TRUE}
set.seed(1)
p_min <- 0.1
levels <- c('yes', 'no')
all_levels <- list(A = levels, B = levels, C = levels)
bn_arr <- list()
for (i in 1:length(nodes)) {
  node <- nodes[i]
  parents <- names(which(true_dag[, node] == 1))
  
  y_values <- runif(max(1, length(parents)), min = p_min)
  n_values <- 1.0 - y_values
  values <- rbind(y_values, n_values) %>% as.vector
  
  bn_arr[[i]] <- gRbase::parray(c(node, parents), levels = all_levels, values = values)
}
cpt <- gRain::compileCPT(bn_arr)
gr_dag <- gRain::grain(cpt)
```

Draw samples from the true DAG. There appears to be a bug somewhere here such that
set.seed does not work when running simulate using gRain. So you will get
irreproducible results for this script.

```{r, error = TRUE}
data <- stats::simulate(gr_dag, nsim = 500, seed = 1)
knitr::kable(head(data))
```

Get all possible graphs. This is quite inefficient. It goes through all possible 
permutations of child, parent, and edge possibilities. Then it only takes the 
unique ones.

```{r, error = TRUE}
tri_val <- expand.grid(c(0, 1), c(0, 1), c(0, 1))
node_perm <- gtools::permutations(length(nodes), length(nodes), nodes)
all_dags <- list()
n <- 1
for (i in 1:nrow(node_perm)) {
  for (j in 1:nrow(node_perm)) {
    for (k in 1:nrow(tri_val)) {
      mat <- matrix(
        0L, 
        nrow = length(nodes), 
        ncol = length(nodes), 
        dimnames = list(node_perm[i, ], node_perm[j, ])
        )
      mat[upper.tri(mat)] <- as.numeric(tri_val[k, ])
      mat <- mat[nodes, nodes]
      
      if (sum(diag(mat)) == 0) {
        bn_gr <- bnlearn::empty.graph(nodes)
        bnlearn::amat(bn_gr) <- mat
        if (bnlearn::acyclic(bn_gr) & bnlearn::directed(bn_gr)) {
          all_dags[[n]] <- mat
          n <- n + 1
        }
      }
    }
  }
}
all_dags <- unique(all_dags)
```

Score all possible DAGs against the simulated data.

```{r, error = TRUE}
scorer <- list(scorer = BNLearnScorer, parameters = list(data = data, type = 'bde'))
score_all_dags <- unlist(lapply(all_dags, ScoreDAG, scorer = scorer))
hash_all_dags <- all_dags %>%
  lapply(rlang::hash) %>% 
  unlist
```

Convert the scores into expected probabilities.

```{r, error = TRUE}
log_z <- LogSumExp(score_all_dags)
log_p <- score_all_dags - log_z
p_true <- exp(log_p)

p_summary <- data.frame(
  hash_dag = hash_all_dags, 
  p_true = p_true
)
```

## Sample from the posterior using MCMC

```{r, error = TRUE}
n_runs <- 4
n_results <- 1000
proposal <- DefaultProposal(p_split_join = 0.33, p_swap_node = 0.33, p_node_move = 0.33)
chains <- list()
set.seed(i)
for (i in 1:n_runs) {
  init_dag <- UniformlySampleDAG(names(data))
  partitioned_nodes <- GetPartitionedNodesFromAdjacencyMatrix(init_dag)
  chain <- SampleChain(n_results, partitioned_nodes, transition = PartitionMCMC,
                       proposal = proposal, scorer = scorer)
  chains[[i]] <- chain
}
```

Looking at the trace of the partition log score.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
col <- c('green', 'red', 'blue')
plot(chains[[1]]$log_score, xlab = 'saves', ylab = 'log(partition score)', type = 'l')
for (i in 2:n_runs) {
  lines(chains[[i]]$log_score, xlab = 'saves', ylab = 'log(partition score)', type = 'l', col = col[i - 1])
}
```

Converting the partitions into DAGs.

```{r, error = TRUE}
n_burnin <- 100
set.seed(1)
dags <- list()
score_dags <- list()
for (i in 1:n_runs) {
  eq_states <- chains[[i]]$state[(1 + n_burnin):n_results]
  dags[[i]] <- lapply(eq_states, SampleDAGFromLabelledPartition, scorer = scorer)
  score_dags[[i]] <- unlist(lapply(dags[[i]], ScoreDAG, scorer = scorer))
}
```

Plot trace of DAG scores.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
true_score <- ScoreDAG(true_dag, scorer)
n_target_saves <- n_results - n_burnin
plot(1:n_target_saves, score_dags[[1]], xlab = 'saves', ylab = 'log(DAG score)', type = 'l')
for (i in 2:n_runs) {
  lines(score_dags[[i]], xlab = 'saves', ylab = 'log(partition score)', type = 'l', col = col[i - 1])
}
```

## Convergence of the score trace

On inspection the score trace for both partitions and DAGs appears to have 
converged across chains. However, I will look at this in more detail for the
quantities of interest below.

Rhat has gaussianity assumptions. What is the best statistical way to check for 
convergence when the partition and DAG scores are not normally distributed?

## Estimating probabilities for each DAG

First we want to check whether the convergence both between and within chains
for the probability of the DAGs of interest. I do this using the Rhat calculation
provided by Gelman in chapter 11 of Bayesian Data Analysis.

```{r, error = TRUE}
unique_sampled_dags <- unique(unlist(dags, recursive = FALSE))
n_within <- (n_results - n_burnin)/2

p_chains <- list()
ssq <- list()
for (i in 1:n_runs) {
  for (m in 1:2) {
    count_dags <- dags[[i]][(1 + (m - 1)*n_within):(m*n_within)] %>%
      lapply(rlang::hash) %>%
      unlist %>%
      table
    sample_p_dags <- count_dags / sum(count_dags)
    p_chains[[2*(i - 1) + m]] <- data.frame(
      chain = i,
      sequence = m,
      hash_dag = names(sample_p_dags),
      p = as.numeric(sample_p_dags)
    )
  }
}
p_chains <- dplyr::bind_rows(p_chains)

# Only going to check convergence for DAGs that appear in all chains. I'm
# assuming that other DAGs have a reasonably low probability, and therefore
# we are less interested in them.
focus_dags <- names(table(p_chains$hash_dag))[table(p_chains$hash_dag) == 2*n_runs]
p_chains <- p_chains %>%
  dplyr::filter(hash_dag %in% focus_dags)

p_marg <- p_chains %>%
  dplyr::group_by(hash_dag) %>%
  dplyr::summarise(p = mean(p))

psi_j <- p_chains %>%
  dplyr::group_by(chain, hash_dag) %>%
  dplyr::summarise(psi_j = mean(p), ssq_j = p*(1.0 - p))

psi_bar <- psi_j %>%
  dplyr::group_by(hash_dag) %>%
  dplyr::summarise(psi_bar = mean(psi_j))

psi <- dplyr::left_join(psi_j, psi_bar, by = 'hash_dag')

bw <- psi %>%
  dplyr::mutate(psi_diff_sq = (psi_j - psi_bar)^2) %>%
  dplyr::group_by(hash_dag) %>%
  dplyr::summarise(
    b = n_within/(2*n_runs - 1)*sum(psi_diff_sq),
    w = mean(ssq_j)
    ) %>%
  dplyr::mutate(
    var = (w*(n_within - 1) + b)/n_within,
    rhat = sqrt(var/w)
    ) %>%
  dplyr::left_join(p_marg, by = 'hash_dag') %>%
  dplyr::left_join(p_summary, by = 'hash_dag') %>%
  dplyr::arrange(desc(p_true))
```

Calculate the effective sample size for each DAG probability. This still doesn't feel quite right.

```{r, error = TRUE}
nmax <- 100
n_eff <- c()
for (i_focus_dag in 1:1) {
  v <- c()
  rho <- c()
  for (t in 0:(nmax - 1)) {
    sum_psi_sq <- 0.0
    for (r in 1:n_runs) {
      for (m in 1:2) {
        dags_j <- dags[[r]][(1 + (m - 1)*n_within):(m*n_within)] %>%
              lapply(rlang::hash) %>%
              unlist
        psi_ij <- dags_j == bw$hash_dag[i_focus_dag]
        
        sum_psi_sq_j <- sum((psi_ij[(t + 1):n_within] - psi_ij[1:(n_within - t)])^2)
        sum_psi_sq <- sum_psi_sq + sum_psi_sq_j
      }
    }
    v_t_const <- 1.0/(2*n_runs*(n_within - t))
    v_t <- v_t_const*sum_psi_sq
    v <- c(v, v_t)
  
    rho_t <- 1.0 - 0.5*v_t/bw$var[i_focus_dag]
    rho <- c(rho, rho_t)
  }
  
  # Only perform a partial sum of rho
  rho_pair_sum <- rho[seq(0, n_within, 2)] + rho[seq(1, n_within, 2)]
  if (all(rho_pair_sum > 0)) {
    # full sum
    rho_part_sum <- sum(rho)
  } else {
    # partial sum
    first_neg_pair <- which(rho_pair_sum < 0)[1]
    rho_part_sum <- sum(rho[1:(first_neg_pair - 1)])
  }
  
  n_eff_dag <- 2*n_runs*n_within/(1 + 2*rho_part_sum)
  n_eff <- c(n_eff, n_eff_dag)
}
bw1 <- bw %>% 
  dplyr::bind_cols(n_eff = n_eff) %>%
  dplyr::mutate(p_sd = sqrt(p*(1.0 - p)/n_eff))

knitr::kable(bw1)
```

## Analysing acceptance rates

Acceptance per proposal.

```{r, error = TRUE}
prop_accept <- dplyr::bind_cols(
  proposal = chains[[1]]$proposal_used, 
  accept = chains[[1]]$accept
  ) %>% 
  dplyr::group_by(proposal) %>% 
  dplyr::summarise(accept = mean(accept))

knitr::kable(prop_accept)
```

Calculate the proportion of split join proposals that are accepted where it 
proposes staying still. This should equal zero now that I've removed this as a 
possible proposal.

```{r, error = TRUE}
dags_col <- simplify2array(dags[[1]])
dags_diff <- dags_col[, , 2:(n_results - n_burnin)] - dags_col[, , 1:(n_results - n_burnin - 1)]
dags_still <- apply(abs(dags_diff), 3, sum) == 0

accepted_sj <- ((chains[[1]]$proposal_used[(1 + n_burnin):(n_results - 1)] == 'split_join')
                & (chains[[1]]$accept[(1 + n_burnin):(n_results - 1)] == 1)
                & (dags_still))
print(mean(accepted_sj))
```

Calculate the proportion of node move proposals that are accepted where it 
proposes staying still...

```{r, error = TRUE}
dags_col <- simplify2array(dags[[1]])
dags_diff <- dags_col[, , 2:(n_results - n_burnin)] - dags_col[, , 1:(n_results - n_burnin - 1)]
dags_still <- apply(abs(dags_diff), 3, sum) == 0

accepted_sj <- ((chains[[1]]$proposal_used[(1 + n_burnin):(n_results - 1)] == 'node_move')
                & (chains[[1]]$accept[(1 + n_burnin):(n_results - 1)] == 1)
                & (dags_still))
print(mean(accepted_sj))
```


Compare to BiDAG..

````{r, error = TRUE}
# library(Matrix)
# bd_data <- tibble::tibble(data) %>% 
#   dplyr::mutate_all(~ as.numeric(. == 'yes')) %>%
#   # as.matrix %>%
#   # as('dtCMatrix') %>%
#   # as.matrix
# 
# # bd_data <- as.numeric(data == 'yes') %>% matrix(ncol = 3)
# score_par <- BiDAG::scoreparameters(
#   'bde', data = bd_data, bdepar = list(chi = 1, edgepf = 1)
#   )
# bidag_res <- BiDAG::partitionMCMC(score_par)


```

<!-- ```{r, error = TRUE, fig.dim = c(7.0, 6.0)} -->
<!-- par(mfrow = c(1, 2)) -->
<!-- for (i in 1:2) { -->
<!--   hash_focus_dag <- compare[i, 'hash_dag'] -->
<!--   focus_dag <- all_dags[[which(hash_all_dags == hash_focus_dag)]] -->

<!--   bn_focus <- bnlearn::empty.graph(nodes) -->
<!--   bnlearn::amat(bn_focus) <- focus_dag -->
<!--   plot(bn_focus) -->
<!-- } -->
<!-- par(mfrow = c(1,1)) -->
<!-- ``` -->

