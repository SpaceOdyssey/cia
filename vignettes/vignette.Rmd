---
title: "Partition MCMC applied to a bnlearn test set"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What is this for?

This shows an example use of dagmc on the bnlearn gaussian.test data set. 

## Setup

```{r setup, message = FALSE}
library(dagmc)
library(bnlearn)
library(dplyr)
library(ggplot2)
```

Get test problem from the bnlearn library.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
data <- bnlearn::alarm[1:100, ]
type <- 'bde'
modelstring <- paste0("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
  "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
  "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
  "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
  "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG]",
  "[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]")
true_graph <- bnlearn::model2network(modelstring)

# # Asia
# data <- bnlearn::asia[1:1000, ]
# type <- 'bde'
# arc_set <- matrix(c('A', 'T',
#                     'T', 'E',
#                     'S', 'B',
#                     'B', 'D',
#                     'L', 'E',
#                     'E', 'D',
#                     'E', 'X'
#                     ),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))

# # Cornary
# data <- bnlearn::coronary
# type <- 'bde'
# arc_set <- matrix(c('Smoking', 'M. Work',
#                     'Smoking', 'Pressure',
#                     'Smoking', 'P. Work',
#                     'M. Work', 'Family',
#                     'M. Work', 'Proteins',
#                     'Pressure', 'Proteins'
#                     ),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))


# # BNlearn learning.test. A discretised problem.
# data <- bnlearn::learning.test[1:1000, ]
# type <- 'bde'
# arc_set <- matrix(c('F', 'E',
#                     'A', 'D',
#                     'B', 'A',
#                     'B', 'E',
#                     'C', 'D'),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))


# # BNlearn Gaussian test. A gaussian problem.
# data <- bnlearn::gaussian.test[1:100, ]
# type <- 'bge'
# true_graph <- bnlearn::empty.graph(names(data))
# arc_set <- matrix(c('A', 'F',
#                     'A', 'C',
#                     'B', 'C',
#                     'B', 'D',
#                     'G', 'F',
#                     'D', 'F',
#                     'E', 'F'),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))
```

Plot true graph.

#```{r, error = TRUE}
# true_graph <- bnlearn::empty.graph(names(data))
# bnlearn::arcs(true_graph) <- arc_set
# true_adj <- true_graph %>%
#   bnlearn::as.igraph() %>%
#   igraph::as_adjacency_matrix() %>%
#   as.matrix
# ```

Show the true DAG from which the data were sampled.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
graphviz.plot(bnlearn::cpdag(true_graph))
```

## Optimise for the known solution

Start with a DAG drawn uniformly from the prior space.

## Posterior sampling.

The aim of dagmc is to sample from the posterior distribution of DAGs. This is
what is done here.

First, a function that calculates the score of a given (node, parents) is
setup.

```{r, error = TRUE}
scorer <- CreateScorer(scorer = BNLearnScorer,
                       data = data, type = type,
                       max_parents = 3,
                       blacklist = NULL,
                       whitelist = NULL,
                       cache = FALSE)

blacklist <- GetLowestScoringEdges(scorer, c = 20)
system.time(
  scorer <- CreateScorer(scorer = BNLearnScorer,
                         data = data, type = type,
                         max_parents = 3,
                         blacklist = blacklist,
                         whitelist = NULL,
                         cache = TRUE)
)
```

I then sample a random DAG from the prior space as an initial state for the MCMC
and get 'n_results' from the sampler. This procedure is done several times
to check that similar results are being reached on each run.

```{r, error = TRUE}
set.seed(1)
n_results <- 1000
n_runs <- 4

init_partitions <- list()
for (i in 1:n_runs) {
  init_dag <- UniformlySampleDAG(names(data))
  
  # Remove edges that disobey white/black listing. I think this breaks the 
  # uniformity from the prior but this will do for now.
  if (!is.null(scorer$blacklist))
    init_dag[which(scorer$blacklist)] <- 0
  
  if (!is.null(scorer$whitelist))
    init_dag[which(scorer$whitelist)] <- 1
  
  init_partitions[[i]] <- GetPartitionedNodesFromAdjacencyMatrix(init_dag)
}

system.time(
  chains <- SampleChains(n_results, init_partitions,
                         transition = PartitionMCMC(),
                         scorer = scorer,
                         n_parallel_chains = n_runs)
)

profvis::profvis(
  chain <- SampleChain(n_results, init_partitions[[1]],
                         transition = PartitionMCMC(),
                         scorer = scorer)
)
```

## Convergence the chains for convergence

Plot trace of the labelled partition log scores. The aim is to check that the 
MCMC is not getting stuck in local maxima for a large number of saves and that 
multiple chains reach the same maxima.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
PlotScoreTrace(chains, ylab = 'log(partition score)', type = 'l')
```

Post process by removing burnin and sampling DAGs from labelled partitions.

```{r, error = TRUE}
n_burnin <- 900
eq_chains <- PostProcessChains(chains, n_burnin)
system.time(
  eq_chains <- SampleChainDAGs(eq_chains, scorer)
)
```

Plot partition score trace after removing burnin.

```{r, error = TRUE}
PlotScoreTrace(eq_chains, ylab = 'log(partition score)', type = 'l')
```

Plot DAG scores.

```{r, error = TRUE}
PlotScoreTrace(eq_chains, attribute = 'log_dag_score', ylab = 'log(DAG score)', type = 'l')
```

## Inference for the Maximum a Posteriori (MAP)

Plot the first found highest scoring DAG.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
flat_dags <- list()
flat_dag_score <- c()
for (i in 1:length(eq_chains)) {
  flat_dags <- c(flat_dags, eq_chains[[i]]$dag)
  flat_dag_score <- c(flat_dag_score, eq_chains[[i]]$log_dag_score)
}

map_dags <- flat_dags[which(flat_dag_score == max(flat_dag_score))]
map_mat <- unique(map_dags)[[1]]  # Just gets the first instance for now.

map_dag <- bnlearn::empty.graph(colnames(data))
bnlearn::amat(map_dag) <- map_mat

plot(bnlearn::cpdag(map_dag))
```

## Analysing acceptance rates

Acceptance per proposal.

```{r, error = TRUE}
prop_accept <- CalculateAcceptanceRates(eq_chains, group_by = 'proposal_used')
knitr::kable(prop_accept)
```



<!-- ## Inference for marginalised edge probabilities  -->

<!-- Plot trace for each marginalised edge probability to check for convergence. -->

<!-- ```{r, error = TRUE, fig.dim = c(8.0, 6.0)} -->
<!-- dags %>% -->
<!--   unlist() %>% -->
<!--   data.frame( -->
<!--     save = rep(1:n_target_saves, each = ncol(data)^2), -->
<!--     parent = rep(colnames(data), times = ncol(data)*n_target_saves), -->
<!--     child = rep(colnames(data), each = ncol(data), times = n_target_saves), -->
<!--     edge = . -->
<!--   ) %>% -->
<!--   dplyr::group_by(parent, child) %>% -->
<!--   dplyr::mutate(edge = cummean(edge)) %>% -->
<!--   ggplot2::ggplot(ggplot2::aes(save, edge, color = interaction(parent, child))) + -->
<!--   ggplot2::geom_path() + -->
<!--   ggplot2::ylab('probability') -->
<!-- ``` -->

<!-- Print marginalised edge probabilities. -->

<!-- ```{r, error = TRUE} -->
<!-- p_edge <- dags %>% -->
<!--   simplify2array() %>% -->
<!--   apply(c(1, 2), mean) -->
<!-- print(p_edge) -->
<!-- ``` -->