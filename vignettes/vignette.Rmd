---
title: "Partition MCMC applied to a bnlearn test set"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## What is this for?

This shows an example use of dagmc on the bnlearn gaussian.test data set. 

## Setup

```{r setup, message = FALSE}
library(dagmc)
library(bnlearn)
library(dplyr)
library(ggplot2)
```

Get test problem from the bnlearn library.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
# data <- bnlearn::gaussian.test[1:100, ]
data <- bnlearn::alarm[1:100, ]

modelstring <- paste0("[HIST|LVF][CVP|LVV][PCWP|LVV][HYP][LVV|HYP:LVF][LVF]",
  "[STKV|HYP:LVF][ERLO][HRBP|ERLO:HR][HREK|ERCA:HR][ERCA][HRSA|ERCA:HR][ANES]",
  "[APL][TPR|APL][ECO2|ACO2:VLNG][KINK][MINV|INT:VLNG][FIO2][PVS|FIO2:VALV]",
  "[SAO2|PVS:SHNT][PAP|PMB][PMB][SHNT|INT:PMB][INT][PRSS|INT:KINK:VTUB][DISC]",
  "[MVS][VMCH|MVS][VTUB|DISC:VMCH][VLNG|INT:KINK:VTUB][VALV|INT:VLNG]",
  "[ACO2|VALV][CCHL|ACO2:ANES:SAO2:TPR][HR|CCHL][CO|HR:STKV][BP|CO:TPR]")
true_graph <- bnlearn::model2network(modelstring)

# true_graph <- bnlearn::empty.graph(names(data))
# arc_set <- matrix(c('A', 'F', 
#                     'A', 'C', 
#                     'B', 'C', 
#                     'B', 'D',
#                     'G', 'F',
#                     'D', 'F',
#                     'E', 'F'),
#                   ncol = 2, byrow = TRUE,
#                   dimnames = list(NULL, c('from', 'to')))
# bnlearn::arcs(true_graph) <- arc_set
# true_adj <- true_graph %>%
#   bnlearn::as.igraph() %>%
#   igraph::as_adjacency_matrix() %>%
#   as.matrix
```

Show the true DAG from which the data were sampled.

```{R, fig.dim = c(6.0, 6.0)}
graphviz.plot(true_graph)
```

## Optimise for the known solution

Start with a DAG drawn uniformly from the prior space.

## Posterior sampling.

The aim of dagmc is to sample from the posterior distribution of DAGs. This is
what is done here.

First, a function that calculates the score of a given (node, parents) is combination
is setup. In this case, I am using the Bayesian Gaussian equivalent score.

```{r, error = TRUE}
scorer <- list(scorer = memo::memo(BNLearnScorer, memo::lru_cache(10000)), 
               parameters = list(data = data, type = 'bde'))
```

I then sample a random DAG from the prior space as an initial state for the MCMC
and get 'n_results' from the sampler. This procedure is done several times
to check that similar results are being reached on each run.

```{r, error = TRUE}
set.seed(1)
n_results <- 2000
n_runs <- parallel::detectCores()

init_partitions <- list()
for (i in 1:n_runs) {
  init_dag <- UniformlySampleDAG(names(data))
  init_partitions[[i]] <- GetPartitionedNodesFromAdjacencyMatrix(init_dag)
}

system.time(
  chains <- SampleChains(n_results, init_partitions, 
                         transition = PartitionMCMC(),
                         scorer = scorer,
                         n_parallel_chains = n_runs)
)
```

## Convergence the chains for convergence

Plot trace of the labelled partition log scores. The aim is to check that the 
MCMC is not getting stuck in local maxima for a large number of saves and that 
multiple chains reach the same maxima.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
PlotScoreTrace(chains, ylab = 'log(partition score)', type = 'l')
```

Removing the first 50% of samples as a burnin. This looks like it's stuck at a 
maxima.


```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
n_burnin <- min(500, as.integer(0.5*n_results))
PlotScoreTrace(chains, burnin = n_burnin, ylab = 'log(partition score)', type = 'l')
```

Convert labelled partitions to DAGs. This is a computationally intensive task, 
so I (i) just do it for the first run, and (ii) only take the samples greater 
than a burnin.

```{r, error = TRUE}
set.seed(1)
dags <- list()
score_dags <- list()
for (i in 1:n_runs) {
  eq_states <- chains[[i]]$state[(1 + n_burnin):n_results]
  dags[[i]] <- lapply(eq_states, SampleDAGFromLabelledPartition, scorer = scorer)
  score_dags[[i]] <- unlist(lapply(dags[[i]], ScoreDAG, scorer = scorer))
}
```

Plot trace of DAG scores.

```{r, error = TRUE, fig.dim = c(6.0, 4.0)}
true_score <- ScoreDAG(true_adj, scorer)
n_target_saves <- n_results - n_burnin
colors <- c('black', 'green', 'red', 'blue', 'brown', 'darkviolet', 'darkgoldenrod', 'deeppink')
plot(1:n_target_saves, score_dags[[1]], xlab = 'saves', ylab = 'log(DAG score)', type = 'l')
for (i in 2:n_runs) {
  lines(score_dags[[i]], xlab = 'saves', ylab = 'log(partition score)', type = 'l', col = colors[i - 1])
}
```

## Inference for the Maximum a Posteriori (MAP)

Plot the first found highest scoring DAG.

```{r, error = TRUE, fig.dim = c(6.0, 6.0)}
flat_score_dags <- unlist(score_dags)
flat_dags <- unlist(dags, recursive = FALSE)
                    
imap <- which(flat_score_dags == max(flat_score_dags))[1]
map_mat <- flat_dags[[imap]]

map_dag <- bnlearn::empty.graph(colnames(data))
amat(map_dag) <- map_mat

plot(bnlearn::cpdag(map_dag))
```

## Analysing acceptance rates

Acceptance per proposal.

```{r, error = TRUE}
chain_info <- list()
for (i in 1:n_runs) {
  chain_info[[i]] <- dplyr::bind_cols(
    proposal_used = unlist(chains[[i]]$proposal_info[n_burnin:(n_results - 1)], use.names = FALSE),
    accept = unlist(chains[[i]]$mcmc_info[n_burnin:(n_results - 1)], use.names = FALSE)
    )
}
chain_info <- dplyr::bind_rows(chain_info)

prop_accept <- chain_info %>% 
  dplyr::group_by(proposal_used) %>% 
  dplyr::summarise(accept = mean(accept))

knitr::kable(prop_accept)
```



<!-- ## Inference for marginalised edge probabilities  -->

<!-- Plot trace for each marginalised edge probability to check for convergence. -->

<!-- ```{r, error = TRUE, fig.dim = c(8.0, 6.0)} -->
<!-- dags %>% -->
<!--   unlist() %>% -->
<!--   data.frame( -->
<!--     save = rep(1:n_target_saves, each = ncol(data)^2), -->
<!--     parent = rep(colnames(data), times = ncol(data)*n_target_saves), -->
<!--     child = rep(colnames(data), each = ncol(data), times = n_target_saves), -->
<!--     edge = . -->
<!--   ) %>% -->
<!--   dplyr::group_by(parent, child) %>% -->
<!--   dplyr::mutate(edge = cummean(edge)) %>% -->
<!--   ggplot2::ggplot(ggplot2::aes(save, edge, color = interaction(parent, child))) + -->
<!--   ggplot2::geom_path() + -->
<!--   ggplot2::ylab('probability') -->
<!-- ``` -->

<!-- Print marginalised edge probabilities. -->

<!-- ```{r, error = TRUE} -->
<!-- p_edge <- dags %>% -->
<!--   simplify2array() %>% -->
<!--   apply(c(1, 2), mean) -->
<!-- print(p_edge) -->
<!-- ``` -->